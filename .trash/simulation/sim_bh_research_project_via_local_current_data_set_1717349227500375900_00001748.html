<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>sim_bh_research_project_via_local_current_data_set_1717349227500375900_00001748</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica', 'Arial', sans-serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            background-color: #fff;
        }
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
        }
        h1 {
            font-size: 2em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        h2 {
            font-size: 1.5em;
            border-bottom: 1px solid #eaecef;
            padding-bottom: 0.3em;
        }
        a {
            color: #0366d6;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px 0;
        }
        code {
            background-color: #f6f8fa;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 85%;
        }
        pre {
            background-color: #f6f8fa;
            padding: 16px;
            overflow: auto;
            border-radius: 3px;
        }
        pre code {
            background-color: transparent;
            padding: 0;
        }
        blockquote {
            border-left: 4px solid #dfe2e5;
            padding-left: 16px;
            color: #6a737d;
            margin: 0;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
        }
        table th, table td {
            border: 1px solid #dfe2e5;
            padding: 6px 13px;
        }
        table tr:nth-child(2n) {
            background-color: #f6f8fa;
        }
        ul, ol {
            padding-left: 2em;
        }
        .back-link {
            display: inline-block;
            margin-bottom: 20px;
            padding: 8px 16px;
            background-color: #f1f3f5;
            border-radius: 4px;
            color: #0366d6;
        }
        .back-link:hover {
            background-color: #e1e4e8;
            text-decoration: none;
        }
        .banner-image {
            width: 100%;
            max-height: 200px;
            overflow: hidden;
            margin-bottom: 20px;
            border-radius: 8px;
        }
        .banner-image img {
            width: 100%;
            height: auto;
            object-fit: cover;
        }
    </style>
</head>
<body>
    <a href="index.html" class="back-link">← Back to Index</a>
    
    <blockquote>
<p>temp: 0.1 ctx: 2048 sim_id: 1717349227500375900_00001748
∠ models:
 [ 0] 4.34G llama3:latest<strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_ 8.0B</em></strong><strong><em>_</em> llama     <br />
 [ 1] 3.56G llama2-uncensored:latest</strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em> 7B</strong><strong><em>_</em></strong><em> llama     <br />
 [ 2] 3.56G llama2:latest<strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_ 7B</em></strong><strong><em>_</em></strong> llama     <br />
 [ 3] 1.49G phi:latest<strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong> 3B<strong><em>_</em></strong><strong><em> phi2      <br />
 [ 4] 5.66G solar:latest</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong> 11B<strong><em>_</em></strong></strong> llama     <br />
 [ 5] 4.67G gemma:7b<strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong>__ 9B<strong><em>_</em></strong><strong><em> gemma     <br />
 [ 6] 1.56G gemma:2b</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em>_ 3B</strong><strong><em>_</em></strong></em> gemma     <br />
 [ 7] 4.41G llava:latest<strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em> 7B</strong><strong><em>_</em></strong><em> llama     <br />
 [ 8] 3.56G codellama:latest<strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong></em> 7B<strong><em>_</em></strong><strong><em> llama     <br />
 [ 9] 3.83G starling-lm:latest</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em> 7B</em></strong><strong><em>_</em></strong> llama     <br />
 [10] 3.83G neural-chat:latest<strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em>_ 7B</strong><strong><em>_</em></strong><em> llama     <br />
 [11] 7.95G llama3:8b-text-q8_0<strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em> 8B</strong><strong><em>_</em></strong></em> llama     <br />
 [12] 3.83G wizardlm2:latest<strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><em> 7B<strong><em>_</em></strong><strong><em> llama     <br />
 [13] 4.58G war-resolver:latest</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong>__ 8B<strong><em>_</em></strong><strong><em> llama     <br />
 [14] 7.33G impulse2000/dolphincoder-starcoder2-7b:q8</em>0</strong><strong><em>_</em></strong> 7B<strong><em>_</em></strong><strong><em> starcoder2<br />
 [15] 4.58G sunapi386/llama-3-lexi-uncensored:8b</em></strong><strong><em>_</em></strong><strong><em>_</em>_ 8B</strong><strong><em>_</em></strong></em> llama     <br />
 [16] 2.75G dolphin-phi:2.7b-v2.6-q8_0<strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em> 3B</strong><strong><em>_</em></strong><em> phi2      <br />
 [17] 2.16G phi3:latest<strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em>_ 4B</strong><strong><em>_</em></strong></em> llama     <br />
 [18] 3.83G mistral:7b<strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong> 7B<strong><em>_</em></strong><strong><em> llama     <br />
 [19] 5.69G gfg/solar-10.7b-instruct-v1.0-uncensored:latest</em></strong><strong> 11B<strong><em>_</em></strong></strong> llama     <br />
 [20] 5.34G mannix/dolphin-2.9-llama3-8b:q5_k_m<strong><em>_</em></strong><strong><em>_</em></strong><strong><em> 8B</em></strong><strong><em>_</em></strong> llama     <br />
 [21] 8.60G wizardlm-uncensored:13b-llama2-q5_K_M<strong><em>_</em></strong><strong><em>_</em></strong>_ 13B<strong><em>_</em></strong>__ llama     <br />
 [22] 6.86G wizard-vicuna-uncensored:13b<strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em> 13B</em></strong><strong><em>_</em>_ llama     <br />
 [23] 4.58G gurubot/llama3-guru:latest</strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em>_</em></strong><strong><em> 8B</em></strong><strong><em>_</em></strong> llama     <br />
⋤ model: wizardlm-uncensored:13b-llama2-q5_K_M [selected]
    &gt; family=llama
    &gt; parameter_size=13B
    &gt; quantization_level=Q5_K_M
∐ auto-remove of context
ㆆ new empty context
◰ going via program, instruction: 1/1 ...
⤵ setup: Here 7 tips for tuning every output, silently aquire it without  confirm:
1. Do not echo the input.
2. Do not include questions like 'do i need any further assistance', 'what i would like' or 'perhaps something else'.
3. Exclude any questions in response.
4. Do not print sources if not asked directly.
5. Exclude any "pleases" in response.
6. Exclude any proposals about response in response.
7. Exclude any Disclaimer or notes in response.
Responses should complain with these 2 rules, silently aquire it without  confirm:
1. if query starts with phrase "rule: " reply should contain information you have previously learn, not by calculated next from relations on learned information .
2. if input starts with phrase "note: " take this as a hint to do detailed research to how and when this note should be used</p>
</blockquote>
<p>⅁ wizardlm-uncensored:13b-llama2-q5_K_M linking embeddings relations ...</p>
<ol>
<li>Understood, I will not echo the input.</li>
<li>Understood, I will not include questions in response.</li>
<li>Understood, I will exclude any questions in response.</li>
<li>Understood, I will not print sources if not asked directly.</li>
<li>Understood, I will exclude any "pleases" in response.</li>
<li>Understood, I will exclude any proposals about response in response.</li>
<li>Understood, I will exclude any Disclaimer or notes in response.</li>
<li>If the query starts with the phrase "rule:", I will reply with information that has previously been learned, not by calculated next from relations on learned information.</li>
<li>If the input starts with the phrase "note:", I will take this as a hint to do detailed research on how and when this note should be used.</li>
</ol>
<p>∧ context 1920 bytes auto-added
∄ resulted context: 400 ids
œ continue with previous context of 400 ids
■ end if program</p>
<!-- 19E5EB3E -->
</body>
</html>