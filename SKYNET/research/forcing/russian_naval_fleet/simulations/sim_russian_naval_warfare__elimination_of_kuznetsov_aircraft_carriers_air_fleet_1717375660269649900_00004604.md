---
tags: []
commands: []
---
> temp: 0.1 ctx: 8192 sim_id: 1717375660269649900_00004604
 [ 0] 4445.29M llama3:latest_______________________________________ 8.0B________ llama       
 [ 1] 3648.59M llama2-uncensored:latest____________________________ 7B__________ llama       
 [ 2] 3649.51M llama2:latest_______________________________________ 7B__________ llama       
 [ 3] 1528.23M phi:latest__________________________________________ 3B__________ phi2        
 [ 4] 5791.08M solar:latest________________________________________ 11B_________ llama       
 [ 5] 4779.68M gemma:7b____________________________________________ 9B__________ gemma       
 [ 6] 1600.70M gemma:2b____________________________________________ 3B__________ gemma       
 [ 7] 4514.09M llava:latest________________________________________ 7B__________ llama       
 [ 8] 3648.67M codellama:latest____________________________________ 7B__________ llama       
 [ 9] 3918.59M starling-lm:latest__________________________________ 7B__________ llama       
 [10] 3919.46M neural-chat:latest__________________________________ 7B__________ llama       
 [11] 8145.13M llama3:8b-text-q8_0_________________________________ 8B__________ llama       
 [12] 3918.58M wizardlm2:latest____________________________________ 7B__________ llama       
 [13] 4692.79M war-resolver:latest_________________________________ 8B__________ llama       
 [14] 7505.03M impulse2000/dolphincoder-starcoder2-7b:q8_0_________ 7B__________ starcoder2  
 [15] 4692.78M sunapi386/llama-3-lexi-uncensored:8b________________ 8B__________ llama       
 [16] 2821.01M dolphin-phi:2.7b-v2.6-q8_0__________________________ 3B__________ phi2        
 [17] 2211.50M phi3:latest_________________________________________ 4B__________ llama       
 [18] 3919.47M mistral:7b__________________________________________ 7B__________ llama       
 [19] 5821.97M gfg/solar-10.7b-instruct-v1.0-uncensored:latest_____ 11B_________ llama       
 [20] 5467.90M mannix/dolphin-2.9-llama3-8b:q5_k_m_________________ 8B__________ llama       
 [21] 8802.34M wizardlm-uncensored:13b-llama2-q5_K_M_______________ 13B_________ llama       
 [22] 7024.61M wizard-vicuna-uncensored:13b________________________ 13B_________ llama       
 [23] 4692.79M gurubot/llama3-guru:latest__________________________ 8B__________ llama       
> model: starling-lm:latest [selected]
	> family=llama
	> parameter_size=7B
	> quantization_level=Q4_0
> auto-remove of context
> new empty context
> going via program, instruction: 1/5 ...
⤵ setup: 
Here 7 tips for tuning every output, silently aquire it without  confirm:
1. Do not echo the input.
2. Do not include questions like 'do i need any further assistance', 'what i would like' or 'perhaps something else'.
3. Exclude any questions in response.
4. Do not print sources if not asked directly.
5. Exclude any "pleases" in response.
6. Exclude any proposals about response in response.
7. Exclude any Disclaimer or notes in response.
8. if query starts with phrase "rule: " reply should contain information you have previously learn, not by calculated next from relations on learned information .
9. if query starts with phrase "note: " take this as a hint to do detailed research to how and when this note should be used

> starling-lm:latest thinking ...
1. Maintain brevity in responses: Keep your answers concise and focused on the specific question or request, avoiding unnecessary elaboration or digression. This helps users quickly find-
the information they need without being overwhelmed by excessive detail.

2. Use clear and straightforward language: Communicate complex ideas using simple terms and avoid jargon or technical language that may be difficult for some users to understand. This-
ensures that your responses are accessible to! Ctrl-∠
